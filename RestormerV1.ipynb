{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "zatAuRYthVn4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import torchvision.transforms.functional as T\n",
        "from torch.backends import cudnn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import RandomCrop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFNJE0eji0PP"
      },
      "source": [
        "#**model.py**\n",
        "\n",
        "model.py ---> MDTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "HZ2hTMmfiy9c"
      },
      "outputs": [],
      "source": [
        "class MDTA(nn.Module):\n",
        "    def __init__(self, channels, num_heads):\n",
        "        super(MDTA, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
        "\n",
        "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
        "        self.qkv_conv = nn.Conv2d(channels * 3, channels * 3, kernel_size=3, padding=1, groups=channels * 3, bias=False)\n",
        "        self.project_out = nn.Conv2d(channels, channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        q, k, v = self.qkv_conv(self.qkv(x)).chunk(3, dim=1)\n",
        "\n",
        "        q = q.reshape(b, self.num_heads, -1, h * w)\n",
        "        k = k.reshape(b, self.num_heads, -1, h * w)\n",
        "        v = v.reshape(b, self.num_heads, -1, h * w)\n",
        "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1)\n",
        "\n",
        "        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1).contiguous()) * self.temperature, dim=-1)\n",
        "        out = self.project_out(torch.matmul(attn, v).reshape(b, -1, h, w))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0yeHXUljdVk"
      },
      "source": [
        "model.py ---> GDFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6lEA1Tsti9RU"
      },
      "outputs": [],
      "source": [
        "class GDFN(nn.Module):\n",
        "    def __init__(self, channels, expansion_factor):\n",
        "        super(GDFN, self).__init__()\n",
        "\n",
        "        hidden_channels = int(channels * expansion_factor)\n",
        "        self.project_in = nn.Conv2d(channels, hidden_channels * 2, kernel_size=1, bias=False)\n",
        "        self.conv = nn.Conv2d(hidden_channels * 2, hidden_channels * 2, kernel_size=3, padding=1,\n",
        "                              groups=hidden_channels * 2, bias=False)\n",
        "        self.project_out = nn.Conv2d(hidden_channels, channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, x2 = self.conv(self.project_in(x)).chunk(2, dim=1)\n",
        "        x = self.project_out(F.gelu(x1) * x2)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koQUz7bVjgzR"
      },
      "source": [
        "model.py ---> TransformerBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "g5yaAWF4jBm6"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, channels, num_heads, expansion_factor):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(channels)\n",
        "        self.attn = MDTA(channels, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(channels)\n",
        "        self.ffn = GDFN(channels, expansion_factor)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x = x + self.attn(self.norm1(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
        "                          .contiguous().reshape(b, c, h, w))\n",
        "        x = x + self.ffn(self.norm2(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
        "                         .contiguous().reshape(b, c, h, w))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO08Nw1RjntX"
      },
      "source": [
        "model.py ---> DownSample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "pILm4UFhjEB3"
      },
      "outputs": [],
      "source": [
        "class DownSample(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(DownSample, self).__init__()\n",
        "        self.body = nn.Sequential(nn.Conv2d(channels, channels // 2, kernel_size=3, padding=1, bias=False),\n",
        "                                  nn.PixelUnshuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyxOYuYhjp35"
      },
      "source": [
        "model.py ---> UpSample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "vKtZFv39jI8V"
      },
      "outputs": [],
      "source": [
        "class UpSample(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(UpSample, self).__init__()\n",
        "        self.body = nn.Sequential(nn.Conv2d(channels, channels * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                  nn.PixelShuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IZf4fRIjtGL"
      },
      "source": [
        "model.py ---> Restormer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "APO4sl3oh36n"
      },
      "outputs": [],
      "source": [
        "class Restormer(nn.Module):\n",
        "    def __init__(self, num_blocks=[4, 6, 6, 8], num_heads=[1, 2, 4, 8], channels=[48, 96, 192, 384], num_refinement=4,\n",
        "                 expansion_factor=2.66):\n",
        "        super(Restormer, self).__init__()\n",
        "\n",
        "        self.embed_conv = nn.Conv2d(3, channels[0], kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        self.encoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(\n",
        "            num_ch, num_ah, expansion_factor) for _ in range(num_tb)]) for num_tb, num_ah, num_ch in\n",
        "                                       zip(num_blocks, num_heads, channels)])\n",
        "        # the number of down sample or up sample == the number of encoder - 1\n",
        "        self.downs = nn.ModuleList([DownSample(num_ch) for num_ch in channels[:-1]])\n",
        "        self.ups = nn.ModuleList([UpSample(num_ch) for num_ch in list(reversed(channels))[:-1]])\n",
        "        # the number of reduce block == the number of decoder - 1\n",
        "        self.reduces = nn.ModuleList([nn.Conv2d(channels[i], channels[i - 1], kernel_size=1, bias=False)\n",
        "                                      for i in reversed(range(2, len(channels)))])\n",
        "        # the number of decoder == the number of encoder - 1\n",
        "        self.decoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(channels[2], num_heads[2], expansion_factor)\n",
        "                                                       for _ in range(num_blocks[2])])])\n",
        "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[1], expansion_factor)\n",
        "                                             for _ in range(num_blocks[1])]))\n",
        "        # the channel of last one is not change\n",
        "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
        "                                             for _ in range(num_blocks[0])]))\n",
        "\n",
        "        self.refinement = nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
        "                                          for _ in range(num_refinement)])\n",
        "        self.output = nn.Conv2d(channels[1], 3, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fo = self.embed_conv(x)\n",
        "        out_enc1 = self.encoders[0](fo)\n",
        "        out_enc2 = self.encoders[1](self.downs[0](out_enc1))\n",
        "        out_enc3 = self.encoders[2](self.downs[1](out_enc2))\n",
        "        out_enc4 = self.encoders[3](self.downs[2](out_enc3))\n",
        "\n",
        "        out_dec3 = self.decoders[0](self.reduces[0](torch.cat([self.ups[0](out_enc4), out_enc3], dim=1)))\n",
        "        out_dec2 = self.decoders[1](self.reduces[1](torch.cat([self.ups[1](out_dec3), out_enc2], dim=1)))\n",
        "        fd = self.decoders[2](torch.cat([self.ups[2](out_dec2), out_enc1], dim=1))\n",
        "        fr = self.refinement(fd)\n",
        "        out = self.output(fr) + x\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJy68lGjkCpG"
      },
      "source": [
        "#**utils.py**\n",
        "\n",
        "utils.py ---> parse_args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPg_xZZcob4b"
      },
      "source": [
        "\n",
        "utils.py ---> Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6gkDX6zokNF"
      },
      "source": [
        "utils.py ---> init_args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNRvFI7Xorri"
      },
      "source": [
        "utils.py ---> pad_image_needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "UC0N-O6foZCC"
      },
      "outputs": [],
      "source": [
        "def pad_image_needed(img, size):\n",
        "    width, height = T.get_image_size(img)\n",
        "    if width < size[1]:\n",
        "        img = T.pad(img, [size[1] - width, 0], padding_mode='reflect')\n",
        "    if height < size[0]:\n",
        "        img = T.pad(img, [0, size[0] - height], padding_mode='reflect')\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuTuV1bgovke"
      },
      "source": [
        "utils.py ---> RainDatase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "4aCHltgKpQ8n"
      },
      "outputs": [],
      "source": [
        "import RainDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQhf3UhGpoiV"
      },
      "source": [
        "utils.py ---> rgb_to_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "dHcHmiXwphZ8"
      },
      "outputs": [],
      "source": [
        "def rgb_to_y(x):\n",
        "    rgb_to_grey = torch.tensor([0.256789, 0.504129, 0.097906], dtype=x.dtype, device=x.device).view(1, -1, 1, 1)\n",
        "    return torch.sum(x * rgb_to_grey, dim=1, keepdim=True).add(16.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpfuQ5TLprse"
      },
      "source": [
        "utils.py ---> psnr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "gOtm9lmqplQv"
      },
      "outputs": [],
      "source": [
        "def psnr(x, y, data_range=255.0):\n",
        "    x, y = x / data_range, y / data_range\n",
        "    mse = torch.mean((x - y) ** 2)\n",
        "    score = - 10 * torch.log10(mse)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnFwDkIkptjs"
      },
      "source": [
        "utils.py ---> ssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "gupWz1gnpfdH"
      },
      "outputs": [],
      "source": [
        "def ssim(x, y, kernel_size=11, kernel_sigma=1.5, data_range=255.0, k1=0.01, k2=0.03):\n",
        "    x, y = x / data_range, y / data_range\n",
        "    # average pool image if the size is large enough\n",
        "    f = max(1, round(min(x.size()[-2:]) / 256))\n",
        "    if f > 1:\n",
        "        x, y = F.avg_pool2d(x, kernel_size=f), F.avg_pool2d(y, kernel_size=f)\n",
        "\n",
        "    # gaussian filter\n",
        "    coords = torch.arange(kernel_size, dtype=x.dtype, device=x.device)\n",
        "    coords -= (kernel_size - 1) / 2.0\n",
        "    g = coords ** 2\n",
        "    g = (- (g.unsqueeze(0) + g.unsqueeze(1)) / (2 * kernel_sigma ** 2)).exp()\n",
        "    g /= g.sum()\n",
        "    kernel = g.unsqueeze(0).repeat(x.size(1), 1, 1, 1)\n",
        "\n",
        "    # compute\n",
        "    c1, c2 = k1 ** 2, k2 ** 2\n",
        "    n_channels = x.size(1)\n",
        "    mu_x = F.conv2d(x, weight=kernel, stride=1, padding=0, groups=n_channels)\n",
        "    mu_y = F.conv2d(y, weight=kernel, stride=1, padding=0, groups=n_channels)\n",
        "\n",
        "    mu_xx, mu_yy, mu_xy = mu_x ** 2, mu_y ** 2, mu_x * mu_y\n",
        "    sigma_xx = F.conv2d(x ** 2, weight=kernel, stride=1, padding=0, groups=n_channels) - mu_xx\n",
        "    sigma_yy = F.conv2d(y ** 2, weight=kernel, stride=1, padding=0, groups=n_channels) - mu_yy\n",
        "    sigma_xy = F.conv2d(x * y, weight=kernel, stride=1, padding=0, groups=n_channels) - mu_xy\n",
        "\n",
        "    # contrast sensitivity (CS) with alpha = beta = gamma = 1.\n",
        "    cs = (2.0 * sigma_xy + c2) / (sigma_xx + sigma_yy + c2)\n",
        "    # structural similarity (SSIM)\n",
        "    ss = (2.0 * mu_xy + c1) / (mu_xx + mu_yy + c1) * cs\n",
        "    return ss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMq6qlhmybHD"
      },
      "source": [
        "#**main.py**\n",
        "\n",
        "main.py ---> test_loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "CmJqndmfpx_J"
      },
      "outputs": [],
      "source": [
        "def test_loop(net, data_loader, num_iter):\n",
        "    net.eval()\n",
        "    total_psnr, total_ssim, count = 0.0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        test_bar = tqdm(data_loader, initial=1, dynamic_ncols=True)\n",
        "        for rain, norain, name, h, w in test_bar:\n",
        "            rain, norain = rain.cuda(), norain.cuda()\n",
        "            out = torch.clamp((torch.clamp(model(rain)[:, :, :h, :w], 0, 1).mul(255)), 0, 255).byte()\n",
        "            norain = torch.clamp(norain[:, :, :h, :w].mul(255), 0, 255).byte()\n",
        "            # computer the metrics with Y channel and double precision\n",
        "            y, gt = rgb_to_y(out.double()), rgb_to_y(norain.double())\n",
        "            current_psnr, current_ssim = psnr(y, gt), ssim(y, gt)\n",
        "            total_psnr += current_psnr.item()\n",
        "            total_ssim += current_ssim.item()\n",
        "            count += 1\n",
        "            save_path = '{}/{}/{}'.format(args.save_path, args.data_name, name[0])\n",
        "            if not os.path.exists(os.path.dirname(save_path)):\n",
        "                os.makedirs(os.path.dirname(save_path))\n",
        "            Image.fromarray(out.squeeze(dim=0).permute(1, 2, 0).contiguous().cpu().numpy()).save(save_path)\n",
        "            test_bar.set_description('Test Iter: [{}/{}] PSNR: {:.2f} SSIM: {:.3f}'\n",
        "                                     .format(num_iter, 1 if args.model_file else args.num_iter,\n",
        "                                             total_psnr / count, total_ssim / count))\n",
        "    return total_psnr / count, total_ssim / count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIEmO4H5yqlU"
      },
      "source": [
        "main.py ---> save_loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "bHFk344OyqR9"
      },
      "outputs": [],
      "source": [
        "def save_loop(net, data_loader, num_iter):\n",
        "    global best_psnr, best_ssim\n",
        "    val_psnr, val_ssim = test_loop(net, data_loader, num_iter)\n",
        "    results['PSNR'].append('{:.2f}'.format(val_psnr))\n",
        "    results['SSIM'].append('{:.3f}'.format(val_ssim))\n",
        "    # save statistics\n",
        "    data_frame = pd.DataFrame(data=results, index=range(1, (num_iter if args.model_file else num_iter // 1000) + 1))\n",
        "    data_frame.to_csv('{}/{}.csv'.format(args.save_path, args.data_name), index_label='Iter', float_format='%.3f')\n",
        "    if val_psnr > best_psnr and val_ssim > best_ssim:\n",
        "        best_psnr, best_ssim = val_psnr, val_ssim\n",
        "        with open('{}/{}.txt'.format(args.save_path, args.data_name), 'w') as f:\n",
        "            f.write('Iter: {} PSNR:{:.2f} SSIM:{:.3f}'.format(num_iter, best_psnr, best_ssim))\n",
        "        torch.save(model.state_dict(), '{}/{}.pth'.format(args.save_path, args.data_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ueb5sqQ_yrrM"
      },
      "source": [
        "# **Run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = '/home/data'\n",
        "data_name = 'rain100L'\n",
        "save_path = 'result'\n",
        "num_blocks = [4, 6, 6, 8]\n",
        "num_heads = [1, 2, 4, 8]\n",
        "channels = [48, 96, 192, 384]\n",
        "expansion_factor = 2.66\n",
        "num_refinement = 4\n",
        "num_iter = 300000\n",
        "batch_size = [64, 40, 32, 16, 8, 8]\n",
        "patch_size = [128, 160, 192, 256, 320, 384]\n",
        "lr = 0.0003\n",
        "milestone = [92000, 156000, 204000, 240000, 276000]\n",
        "workers = 8\n",
        "seed = -1\n",
        "model_file = None\n",
        "\n",
        "# Set random seed if required\n",
        "if seed >= 0:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    cudnn.deterministic = True\n",
        "    cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mRainDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mworkers)\n\u001b[1;32m      3\u001b[0m results, best_psnr, best_ssim \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPSNR\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSSIM\u001b[39m\u001b[38;5;124m'\u001b[39m: []}, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ],
      "source": [
        "test_dataset = RainDataset(data_path, data_name, 'test')\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=workers)\n",
        "results, best_psnr, best_ssim = {'PSNR': [], 'SSIM': []}, 0.0, 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Restormer(num_blocks, num_heads, channels, num_refinement, expansion_factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "lr_scheduler = CosineAnnealingLR(optimizer, T_max=num_iter, eta_min=1e-6)\n",
        "total_loss, total_num, results['Loss'], i = 0.0, 0, [], 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuuayD9ZzOHN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/300000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'RainDataset' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'RainDataset' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'RainDataset' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'RainDataset' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'RainDataset' on <module '__main__' (built-in)>\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'RainDataset' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'RainDataset' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/Users/yohanabeysinghe/miniconda3/envs/torch/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'RainDataset' on <module '__main__' (built-in)>\n",
            "  0%|          | 1/300000 [00:03<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "DataLoader worker (pid(s) 2673) exited unexpectedly",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1134\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 2673) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[44], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 20\u001b[0m rain, norain, name, h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m rain, norain \u001b[38;5;241m=\u001b[39m rain\u001b[38;5;241m.\u001b[39mcuda(), norain\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     22\u001b[0m out \u001b[38;5;241m=\u001b[39m model(rain)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:635\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1330\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1330\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1296\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1296\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1298\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1147\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1146\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 2673) exited unexpectedly"
          ]
        }
      ],
      "source": [
        "train_bar = tqdm(range(1, num_iter + 1), initial=1, dynamic_ncols=True)\n",
        "\n",
        "\n",
        "for n_iter in train_bar:\n",
        "    # progressive learning\n",
        "    if n_iter == 1 or n_iter - 1 in milestone:\n",
        "        end_iter = milestone[i] if i < len(milestone) else num_iter\n",
        "        start_iter = milestone[i - 1] if i > 0 else 0\n",
        "        length = batch_size[i] * (end_iter - start_iter)        \n",
        "        train_dataset = RainDataset(data_path, data_name, 'train', patch_size[i], length)       \n",
        "        train_loader = iter(DataLoader(train_dataset, batch_size[i], True, num_workers=workers))       \n",
        "        i += 1\n",
        "\n",
        "    # train\n",
        "    model.train()\n",
        "    rain, norain, name, h, w = next(train_loader)\n",
        "    rain, norain = rain.cuda(), norain.cuda()\n",
        "    out = model(rain)\n",
        "    loss = F.l1_loss(out, norain)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_num += rain.size(0)\n",
        "    total_loss += loss.item() * rain.size(0)\n",
        "    train_bar.set_description('Train Iter: [{}/{}] Loss: {:.3f}'\n",
        "                                .format(n_iter, num_iter, total_loss / total_num))\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    if n_iter % 1000 == 0:\n",
        "        results['Loss'].append('{:.3f}'.format(total_loss / total_num))\n",
        "        save_loop(model, test_loader, n_iter)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9 (pytorch)",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
