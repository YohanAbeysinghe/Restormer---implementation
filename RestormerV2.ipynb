{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyexYBYiXw8s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision.transforms.functional as T\n",
        "from torchvision.transforms import RandomCrop\n",
        "\n",
        "from rain_dataset import RainDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp2V-NPxYRaU"
      },
      "outputs": [],
      "source": [
        "class MDTA(nn.Module):\n",
        "    def __init__(self, channels, num_heads):\n",
        "        super(MDTA, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
        "\n",
        "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
        "        self.qkv_conv = nn.Conv2d(channels * 3, channels * 3, kernel_size=3, padding=1, groups=channels * 3, bias=False)\n",
        "        self.project_out = nn.Conv2d(channels, channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        q, k, v = self.qkv_conv(self.qkv(x)).chunk(3, dim=1)\n",
        "\n",
        "        q = q.reshape(b, self.num_heads, -1, h * w)\n",
        "        k = k.reshape(b, self.num_heads, -1, h * w)\n",
        "        v = v.reshape(b, self.num_heads, -1, h * w)\n",
        "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1)\n",
        "\n",
        "        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1).contiguous()) * self.temperature, dim=-1)\n",
        "        out = self.project_out(torch.matmul(attn, v).reshape(b, -1, h, w))\n",
        "        return out\n",
        "\n",
        "\n",
        "class GDFN(nn.Module):\n",
        "    def __init__(self, channels, expansion_factor):\n",
        "        super(GDFN, self).__init__()\n",
        "\n",
        "        hidden_channels = int(channels * expansion_factor)\n",
        "        self.project_in = nn.Conv2d(channels, hidden_channels * 2, kernel_size=1, bias=False)\n",
        "        self.conv = nn.Conv2d(hidden_channels * 2, hidden_channels * 2, kernel_size=3, padding=1,\n",
        "                              groups=hidden_channels * 2, bias=False)\n",
        "        self.project_out = nn.Conv2d(hidden_channels, channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, x2 = self.conv(self.project_in(x)).chunk(2, dim=1)\n",
        "        x = self.project_out(F.gelu(x1) * x2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, channels, num_heads, expansion_factor):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(channels)\n",
        "        self.attn = MDTA(channels, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(channels)\n",
        "        self.ffn = GDFN(channels, expansion_factor)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x = x + self.attn(self.norm1(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
        "                          .contiguous().reshape(b, c, h, w))\n",
        "        x = x + self.ffn(self.norm2(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
        "                         .contiguous().reshape(b, c, h, w))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(DownSample, self).__init__()\n",
        "        self.body = nn.Sequential(nn.Conv2d(channels, channels // 2, kernel_size=3, padding=1, bias=False),\n",
        "                                  nn.PixelUnshuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(UpSample, self).__init__()\n",
        "        self.body = nn.Sequential(nn.Conv2d(channels, channels * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                  nn.PixelShuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "\n",
        "\n",
        "class Restormer(nn.Module):\n",
        "    def __init__(self, num_blocks=[4, 6, 6, 8], num_heads=[1, 2, 4, 8], channels=[48, 96, 192, 384], num_refinement=4,\n",
        "                 expansion_factor=2.66):\n",
        "        super(Restormer, self).__init__()\n",
        "\n",
        "        self.embed_conv = nn.Conv2d(3, channels[0], kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        self.encoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(\n",
        "            num_ch, num_ah, expansion_factor) for _ in range(num_tb)]) for num_tb, num_ah, num_ch in\n",
        "                                       zip(num_blocks, num_heads, channels)])\n",
        "        # the number of down sample or up sample == the number of encoder - 1\n",
        "        self.downs = nn.ModuleList([DownSample(num_ch) for num_ch in channels[:-1]])\n",
        "        self.ups = nn.ModuleList([UpSample(num_ch) for num_ch in list(reversed(channels))[:-1]])\n",
        "        # the number of reduce block == the number of decoder - 1\n",
        "        self.reduces = nn.ModuleList([nn.Conv2d(channels[i], channels[i - 1], kernel_size=1, bias=False)\n",
        "                                      for i in reversed(range(2, len(channels)))])\n",
        "        # the number of decoder == the number of encoder - 1\n",
        "        self.decoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(channels[2], num_heads[2], expansion_factor)\n",
        "                                                       for _ in range(num_blocks[2])])])\n",
        "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[1], expansion_factor)\n",
        "                                             for _ in range(num_blocks[1])]))\n",
        "        # the channel of last one is not change\n",
        "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
        "                                             for _ in range(num_blocks[0])]))\n",
        "\n",
        "        self.refinement = nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
        "                                          for _ in range(num_refinement)])\n",
        "        self.output = nn.Conv2d(channels[1], 3, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fo = self.embed_conv(x)\n",
        "        out_enc1 = self.encoders[0](fo)\n",
        "        out_enc2 = self.encoders[1](self.downs[0](out_enc1))\n",
        "        out_enc3 = self.encoders[2](self.downs[1](out_enc2))\n",
        "        out_enc4 = self.encoders[3](self.downs[2](out_enc3))\n",
        "\n",
        "        out_dec3 = self.decoders[0](self.reduces[0](torch.cat([self.ups[0](out_enc4), out_enc3], dim=1)))\n",
        "        out_dec2 = self.decoders[1](self.reduces[1](torch.cat([self.ups[1](out_dec3), out_enc2], dim=1)))\n",
        "        fd = self.decoders[2](torch.cat([self.ups[2](out_dec2), out_enc1], dim=1))\n",
        "        fr = self.refinement(fd)\n",
        "        out = self.output(fr) + x\n",
        "        return out\n",
        "\n",
        "\n",
        "def initialize_args():\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            self.data_path = 'data'\n",
        "            self.data_name = 'rain100L'\n",
        "            self.save_path = 'result'\n",
        "            self.num_blocks = [2, 3, 3, 4] #[4, 6, 6, 8]\n",
        "            self.num_heads = [1, 2, 4, 8]\n",
        "            self.channels = [48, 96, 192, 384]\n",
        "            self.expansion_factor = 2.66\n",
        "            self.num_refinement = 4\n",
        "            self.num_iter = 300000\n",
        "            self.batch_size = [32, 20, 16, 8, 4, 4]   #[64, 40, 32, 16, 8, 8] #[40, 32, 16, 8, 8]\n",
        "            self.patch_size = [32, 40, 48, 64, 80, 96] #[64, 80, 96, 128, 160, 192] #[128, 160, 192, 256, 320, 384] #[160, 192, 256, 320, 384]\n",
        "            self.lr = 0.0003\n",
        "            self.milestone = [92000, 156000, 204000, 240000, 276000] #[92000, 156000, 204000, 240000, 276000]\n",
        "            self.workers = 8\n",
        "            self.seed = -1\n",
        "            self.model_file = None\n",
        "\n",
        "    return Args()\n",
        "\n",
        "\n",
        "\n",
        "def rgb_to_y(x):\n",
        "    rgb_to_grey = torch.tensor([0.256789, 0.504129, 0.097906], dtype=x.dtype, device=x.device).view(1, -1, 1, 1)\n",
        "    return torch.sum(x * rgb_to_grey, dim=1, keepdim=True).add(16.0)\n",
        "\n",
        "\n",
        "def psnr(x, y, data_range=255.0):\n",
        "    x, y = x / data_range, y / data_range\n",
        "    mse = torch.mean((x - y) ** 2)\n",
        "    score = - 10 * torch.log10(mse)\n",
        "    return score\n",
        "\n",
        "\n",
        "def ssim(x, y, kernel_size=11, kernel_sigma=1.5, data_range=255.0, k1=0.01, k2=0.03):\n",
        "    x, y = x / data_range, y / data_range\n",
        "    # average pool image if the size is large enough\n",
        "    f = max(1, round(min(x.size()[-2:]) / 256))\n",
        "    if f > 1:\n",
        "        x, y = F.avg_pool2d(x, kernel_size=f), F.avg_pool2d(y, kernel_size=f)\n",
        "\n",
        "    # gaussian filter\n",
        "    coords = torch.arange(kernel_size, dtype=x.dtype, device=x.device)\n",
        "    coords -= (kernel_size - 1) / 2.0\n",
        "    g = coords ** 2\n",
        "    g = (- (g.unsqueeze(0) + g.unsqueeze(1)) / (2 * kernel_sigma ** 2)).exp()\n",
        "    g /= g.sum()\n",
        "    kernel = g.unsqueeze(0).repeat(x.size(1), 1, 1, 1)\n",
        "\n",
        "    # compute\n",
        "    c1, c2 = k1 ** 2, k2 ** 2\n",
        "    n_channels = x.size(1)\n",
        "    mu_x = F.conv2d(x, weight=kernel, stride=1, padding=0, groups=n_channels)\n",
        "    mu_y = F.conv2d(y, weight=kernel, stride=1, padding=0, groups=n_channels)\n",
        "\n",
        "    mu_xx, mu_yy, mu_xy = mu_x ** 2, mu_y ** 2, mu_x * mu_y\n",
        "    sigma_xx = F.conv2d(x ** 2, weight=kernel, stride=1, padding=0, groups=n_channels) - mu_xx\n",
        "    sigma_yy = F.conv2d(y ** 2, weight=kernel, stride=1, padding=0, groups=n_channels) - mu_yy\n",
        "    sigma_xy = F.conv2d(x * y, weight=kernel, stride=1, padding=0, groups=n_channels) - mu_xy\n",
        "\n",
        "    # contrast sensitivity (CS) with alpha = beta = gamma = 1.\n",
        "    cs = (2.0 * sigma_xy + c2) / (sigma_xx + sigma_yy + c2)\n",
        "    # structural similarity (SSIM)\n",
        "    ss = (2.0 * mu_xy + c1) / (mu_xx + mu_yy + c1) * cs\n",
        "    return ss.mean()\n",
        "\n",
        "\n",
        "\n",
        "def test_loop(net, data_loader, num_iter):\n",
        "    net.eval()\n",
        "    total_psnr, total_ssim, count = 0.0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        test_bar = tqdm(data_loader, initial=1, dynamic_ncols=True)\n",
        "        for rain, norain, name, h, w in test_bar:\n",
        "            rain, norain = rain.cuda(), norain.cuda()\n",
        "            out = torch.clamp((torch.clamp(model(rain)[:, :, :h, :w], 0, 1).mul(255)), 0, 255).byte()\n",
        "            norain = torch.clamp(norain[:, :, :h, :w].mul(255), 0, 255).byte()\n",
        "            # computer the metrics with Y channel and double precision\n",
        "            y, gt = rgb_to_y(out.double()), rgb_to_y(norain.double())\n",
        "            current_psnr, current_ssim = psnr(y, gt), ssim(y, gt)\n",
        "            total_psnr += current_psnr.item()\n",
        "            total_ssim += current_ssim.item()\n",
        "            count += 1\n",
        "            save_path = '{}/{}/{}'.format(args.save_path, args.data_name, name[0])\n",
        "            if not os.path.exists(os.path.dirname(save_path)):\n",
        "                os.makedirs(os.path.dirname(save_path))\n",
        "            Image.fromarray(out.squeeze(dim=0).permute(1, 2, 0).contiguous().cpu().numpy()).save(save_path)\n",
        "            test_bar.set_description('Test Iter: [{}/{}] PSNR: {:.2f} SSIM: {:.3f}'\n",
        "                                     .format(num_iter, 1 if args.model_file else args.num_iter,\n",
        "                                             total_psnr / count, total_ssim / count))\n",
        "    return total_psnr / count, total_ssim / count\n",
        "\n",
        "\n",
        "def save_loop(net, data_loader, num_iter):\n",
        "    global best_psnr, best_ssim\n",
        "    val_psnr, val_ssim = test_loop(net, data_loader, num_iter)\n",
        "    results['PSNR'].append('{:.2f}'.format(val_psnr))\n",
        "    results['SSIM'].append('{:.3f}'.format(val_ssim))\n",
        "    # save statistics\n",
        "    data_frame = pd.DataFrame(data=results, index=range(1, (num_iter if args.model_file else num_iter // 1000) + 1))\n",
        "    data_frame.to_csv('{}/{}.csv'.format(args.save_path, args.data_name), index_label='Iter', float_format='%.3f')\n",
        "    if val_psnr > best_psnr and val_ssim > best_ssim:\n",
        "        best_psnr, best_ssim = val_psnr, val_ssim\n",
        "        with open('{}/{}.txt'.format(args.save_path, args.data_name), 'w') as f:\n",
        "            f.write('Iter: {} PSNR:{:.2f} SSIM:{:.3f}'.format(num_iter, best_psnr, best_ssim))\n",
        "        torch.save(model.state_dict(), '{}/{}.pth'.format(args.save_path, args.data_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "PkKTiPiVYXPs",
        "outputId": "8eb5559a-176c-408b-ca1a-c4cd074ad482"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 1/300000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Iter: [1000/300000] Loss: 0.018:   0%|          | 1000/300000 [02:43<10:56:38,  7.59it/s]\n",
            "  1%|          | 1/100 [00:01<?, ?it/s]\n",
            "Train Iter: [1000/300000] Loss: 0.018:   0%|          | 1000/300000 [02:44<13:39:05,  6.08it/s]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "Caught NameError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/content/drive/MyDrive/Yohan/Restormer - Implementation/Restormer_from_scratch/rain_dataset.py\", line 56, in __getitem__\n    rain = F.pad(rain, (0, pad_w, 0, pad_h), 'reflect')\nNameError: name 'F' is not defined\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3021e830ca66>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0msave_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-05c517a5756f>\u001b[0m in \u001b[0;36msave_loop\u001b[0;34m(net, data_loader, num_iter)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest_psnr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_ssim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mval_psnr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ssim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PSNR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_psnr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SSIM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_ssim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-05c517a5756f>\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(net, data_loader, num_iter)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mtest_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0mrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: Caught NameError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/content/drive/MyDrive/Yohan/Restormer - Implementation/Restormer_from_scratch/rain_dataset.py\", line 56, in __getitem__\n    rain = F.pad(rain, (0, pad_w, 0, pad_h), 'reflect')\nNameError: name 'F' is not defined\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    #args = parse_args()\n",
        "    args = initialize_args()\n",
        "    test_dataset = RainDataset(args.data_path, args.data_name, 'test')\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=args.workers)\n",
        "\n",
        "    results, best_psnr, best_ssim = {'PSNR': [], 'SSIM': []}, 0.0, 0.0\n",
        "    model = Restormer(args.num_blocks, args.num_heads, args.channels, args.num_refinement, args.expansion_factor).cuda()\n",
        "    if args.model_file:\n",
        "        model.load_state_dict(torch.load(args.model_file))\n",
        "        save_loop(model, test_loader, 1)\n",
        "    else:\n",
        "        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
        "        lr_scheduler = CosineAnnealingLR(optimizer, T_max=args.num_iter, eta_min=1e-6)\n",
        "        total_loss, total_num, results['Loss'], i = 0.0, 0, [], 0\n",
        "        train_bar = tqdm(range(1, args.num_iter + 1), initial=1, dynamic_ncols=True)\n",
        "        for n_iter in train_bar:\n",
        "            # progressive learning\n",
        "            if n_iter == 1 or n_iter - 1 in args.milestone:\n",
        "                end_iter = args.milestone[i] if i < len(args.milestone) else args.num_iter\n",
        "                start_iter = args.milestone[i - 1] if i > 0 else 0\n",
        "                length = args.batch_size[i] * (end_iter - start_iter)\n",
        "                train_dataset = RainDataset(args.data_path, args.data_name, 'train', args.patch_size[i], length)\n",
        "                train_loader = iter(DataLoader(train_dataset, args.batch_size[i], True, num_workers=args.workers))\n",
        "                i += 1\n",
        "            # train\n",
        "            model.train()\n",
        "            rain, norain, name, h, w = next(train_loader)\n",
        "            rain, norain = rain.cuda(), norain.cuda()\n",
        "\n",
        "            out = model(rain)\n",
        "            loss = F.l1_loss(out, norain)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_num += rain.size(0)\n",
        "            total_loss += loss.item() * rain.size(0)\n",
        "            train_bar.set_description('Train Iter: [{}/{}] Loss: {:.3f}'\n",
        "                                      .format(n_iter, args.num_iter, total_loss / total_num))\n",
        "\n",
        "            lr_scheduler.step()\n",
        "            if n_iter % 1000 == 0:\n",
        "                results['Loss'].append('{:.3f}'.format(total_loss / total_num))\n",
        "                save_loop(model, test_loader, n_iter)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
